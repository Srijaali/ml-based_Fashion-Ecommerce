"""
Collaborative Filtering Training - Kaggle GPU Optimized Version
Trains on 400k users using Tesla T4 GPU (15-20 minutes)

üöÄ USAGE:
   1. Create new Kaggle Notebook
   2. Copy this entire script
   3. Select GPU (right sidebar)
   4. Run all cells
   5. Download results from /kaggle/working/

‚öôÔ∏è  EXPECTED:
   ‚Ä¢ Training time: 15-20 minutes
   ‚Ä¢ Users processed: 400,000
   ‚Ä¢ Model quality: 85+ / 100
   ‚Ä¢ User-based recommendations: ~2.5M
   ‚Ä¢ Item-based recommendations: ~133k

üìä OUTPUT FILES (in /kaggle/working/):
   ‚Ä¢ user_based_recommendations.parquet (2.5M rows)
   ‚Ä¢ item_based_recommendations.parquet
   ‚Ä¢ user_embeddings.parquet
   ‚Ä¢ item_embeddings.parquet
   ‚Ä¢ user_id_mapping.parquet
   ‚Ä¢ article_id_mapping.parquet
"""

import os
import sys
import pickle
import logging
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION
# ============================================================================

KAGGLE_MODE = os.path.exists('/kaggle')

if KAGGLE_MODE:
    INPUT_DIR = Path('/kaggle/input/fashion-etl-data')
    OUTPUT_DIR = Path('/kaggle/working')
else:
    # Fallback for local testing
    INPUT_DIR = Path('data/etl/output')
    OUTPUT_DIR = Path('data/recommendations')

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# HYPERPARAMETERS (tune these)
TRAIN_USER_COUNT = 400000  # Set to 400k for Kaggle, 5k for local testing
N_COMPONENTS = 50          # Latent factors (50 recommended)
SVD_ITERATIONS = 100       # SVD iterations (100 recommended)
SIMILARITY_METHOD = 'cosine'  # 'cosine' or 'euclidean'
KNN_NEIGHBORS = 20         # Neighbors to consider (20 recommended)
MIN_SIMILARITY = 0.1       # Minimum similarity threshold

# GPU SETTINGS
USE_GPU = True
TRY_CUML = True

# ============================================================================
# SETUP LOGGING
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# GPU DETECTION & ACCELERATION
# ============================================================================

gpu_available = False
cupy_available = False
cuml_available = False

if USE_GPU:
    try:
        import cupy as cp
        cupy_available = True
        logger.info("‚úÖ CuPy detected (GPU array operations)")
    except ImportError:
        logger.info("‚ö†Ô∏è  CuPy not available (using NumPy)")
    
    if TRY_CUML:
        try:
            from cuml.decomposition import TruncatedSVD as cuMLSVD
            from cuml.neighbors import NearestNeighbors as cuMLKNN
            cuml_available = True
            logger.info("‚úÖ cuML detected (GPU accelerated SVD & KNN)")
        except ImportError:
            logger.info("‚ö†Ô∏è  cuML not available (using scikit-learn)")

logger.info(f"üñ•Ô∏è  GPU Acceleration: CuPy={cupy_available}, cuML={cuml_available}")

# ============================================================================
# DATA LOADING
# ============================================================================

def load_data():
    """Load interaction data from parquet file"""
    logger.info("üìÇ Loading interaction data...")
    
    parquet_file = INPUT_DIR / 'user_item_interactions.parquet'
    
    if not parquet_file.exists():
        # Try alternative path
        parquet_file = INPUT_DIR.parent / 'user_item_interactions.parquet'
    
    if not parquet_file.exists():
        raise FileNotFoundError(f"Data file not found at {parquet_file}")
    
    df = pd.read_parquet(parquet_file)
    logger.info(f"‚úÖ Loaded {len(df):,} interactions")
    logger.info(f"   Unique users: {df['customer_id'].nunique():,}")
    logger.info(f"   Unique items: {df['article_id'].nunique():,}")
    
    return df

# ============================================================================
# DATA SAMPLING & PREPROCESSING
# ============================================================================

def sample_and_preprocess(df, max_users=TRAIN_USER_COUNT):
    """Sample users and preprocess data"""
    logger.info(f"üîÑ Preprocessing data (max {max_users:,} users)...")
    
    # Sample top users by interaction count
    user_counts = df['customer_id'].value_counts()
    if len(user_counts) > max_users:
        top_users = user_counts.head(max_users).index
        df = df[df['customer_id'].isin(top_users)]
        logger.info(f"   Sampled top {len(top_users):,} users")
    
    # Use time_weighted_score if available, else purchase_count
    if 'time_weighted_score' in df.columns:
        df['weight'] = df['time_weighted_score']
        logger.info("   Using time_weighted_score as weight")
    else:
        df['weight'] = df['purchase_count']
        logger.info("   Using purchase_count as weight")
    
    # Create user and item mappings
    users = df['customer_id'].unique()
    items = df['article_id'].unique()
    
    user_idx = {uid: i for i, uid in enumerate(users)}
    item_idx = {aid: i for i, aid in enumerate(items)}
    
    logger.info(f"‚úÖ Final dataset:")
    logger.info(f"   Users: {len(users):,}")
    logger.info(f"   Items: {len(items):,}")
    logger.info(f"   Interactions: {len(df):,}")
    
    return df, user_idx, item_idx, users, items

# ============================================================================
# MATRIX CONSTRUCTION
# ============================================================================

def build_interaction_matrix(df, user_idx, item_idx):
    """Build sparse user-item interaction matrix"""
    logger.info("üî® Building interaction matrix...")
    
    row = df['customer_id'].map(user_idx).values
    col = df['article_id'].map(item_idx).values
    data = df['weight'].values
    
    matrix = csr_matrix(
        (data, (row, col)),
        shape=(len(user_idx), len(item_idx))
    )
    
    logger.info(f"‚úÖ Matrix created: {matrix.shape[0]:,} x {matrix.shape[1]:,}")
    logger.info(f"   Sparsity: {(1 - matrix.nnz / (matrix.shape[0] * matrix.shape[1])) * 100:.2f}%")
    logger.info(f"   Density: {matrix.nnz / (matrix.shape[0] * matrix.shape[1]) * 100:.6f}%")
    
    return matrix

# ============================================================================
# SVD DECOMPOSITION
# ============================================================================

def fit_svd(matrix, n_components=N_COMPONENTS, n_iter=SVD_ITERATIONS):
    """Fit SVD decomposition"""
    logger.info(f"üìä SVD Decomposition ({n_components} components, {n_iter} iterations)...")
    
    if cuml_available:
        logger.info("   Using cuML GPU-accelerated SVD...")
        try:
            svd = cuMLSVD(
                n_components=n_components,
                n_iter=n_iter,
                whiten=False,
                random_state=42
            )
            U = svd.fit_transform(matrix.astype(np.float32))
            V = svd.components_
            logger.info("   ‚úÖ cuML SVD completed")
        except Exception as e:
            logger.warning(f"   cuML failed: {e}, falling back to sklearn")
            cuml_available = False
    
    if not cuml_available:
        logger.info("   Using scikit-learn SVD...")
        svd = TruncatedSVD(
            n_components=n_components,
            n_iter=n_iter,
            random_state=42
        )
        svd.fit(matrix)
        U = svd.transform(matrix)
        V = svd.components_
        logger.info(f"   ‚úÖ Variance explained: {svd.explained_variance_ratio_.sum() * 100:.2f}%")
    
    return U, V, svd

# ============================================================================
# SIMILARITY COMPUTATION
# ============================================================================

def compute_similarities(embeddings, method='cosine'):
    """Compute similarity matrix"""
    logger.info(f"üîó Computing {method} similarities...")
    
    if method == 'cosine':
        # Normalize embeddings
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        embeddings_norm = embeddings / (norms + 1e-10)
        
        # Compute cosine similarity
        similarity = np.dot(embeddings_norm, embeddings_norm.T)
    else:
        from sklearn.metrics.pairwise import euclidean_distances
        distances = euclidean_distances(embeddings)
        similarity = 1 / (1 + distances)
    
    # Zero out self-similarities and low values
    np.fill_diagonal(similarity, 0)
    similarity[similarity < MIN_SIMILARITY] = 0
    
    logger.info(f"‚úÖ Similarity matrix computed: {similarity.shape}")
    logger.info(f"   Non-zero similarities: {(similarity > 0).sum():,}")
    
    return similarity

# ============================================================================
# KNN INDEX BUILDING
# ============================================================================

def build_knn_index(embeddings, n_neighbors=KNN_NEIGHBORS, algorithm='auto'):
    """Build KNN index for fast similarity search"""
    logger.info(f"üèóÔ∏è  Building KNN index ({n_neighbors} neighbors, {algorithm})...")
    
    if cuml_available and algorithm != 'brute':
        try:
            knn = cuMLKNN(n_neighbors=n_neighbors, algorithm=algorithm)
            knn.fit(embeddings.astype(np.float32))
            logger.info("   ‚úÖ cuML KNN index built")
            return knn
        except Exception as e:
            logger.warning(f"   cuML KNN failed: {e}, using sklearn")
    
    knn = NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm)
    knn.fit(embeddings)
    logger.info("   ‚úÖ sklearn KNN index built")
    
    return knn

# ============================================================================
# RECOMMENDATION GENERATION
# ============================================================================

def generate_user_based_recommendations(user_embeddings, user_idx, item_idx, 
                                       matrix, knn_index, n_recs=20):
    """Generate user-based collaborative filtering recommendations"""
    logger.info(f"üë• Generating user-based recommendations ({n_recs} per user)...")
    
    recommendations = []
    
    for idx, (user_id, user_idx_val) in enumerate(user_idx.items()):
        if (idx + 1) % 50000 == 0:
            logger.info(f"   Progress: {idx + 1:,} / {len(user_idx):,}")
        
        # Find similar users
        user_vector = user_embeddings[user_idx_val:user_idx_val+1]
        
        try:
            distances, indices = knn_index.kneighbors(user_vector, n_neighbors=min(50, len(user_idx)))
            similar_users = indices[0][1:]  # Exclude self
        except:
            continue
        
        # Get items from similar users
        user_items = set(matrix[user_idx_val].nonzero()[1])
        candidate_items = {}
        
        for sim_user_idx in similar_users[:30]:  # Limit to top 30 similar users
            sim_items = matrix[sim_user_idx].nonzero()[1]
            
            for item_idx_val in sim_items:
                if item_idx_val not in user_items:
                    score = matrix[sim_user_idx, item_idx_val]
                    if item_idx_val in candidate_items:
                        candidate_items[item_idx_val] += score
                    else:
                        candidate_items[item_idx_val] = score
        
        # Sort and take top N
        if candidate_items:
            top_items = sorted(candidate_items.items(), key=lambda x: x[1], reverse=True)[:n_recs]
            
            for rank, (item_idx_val, score) in enumerate(top_items, 1):
                article_id = list(item_idx.keys())[list(item_idx.values()).index(item_idx_val)]
                
                recommendations.append({
                    'user_id': user_id,
                    'article_id': article_id,
                    'rank': rank,
                    'score': float(score),
                    'method': 'user_based'
                })
    
    logger.info(f"‚úÖ Generated {len(recommendations):,} user-based recommendations")
    return pd.DataFrame(recommendations)

def generate_item_based_recommendations(item_embeddings, user_idx, item_idx,
                                       matrix, n_recs=20):
    """Generate item-based collaborative filtering recommendations"""
    logger.info(f"üì¶ Generating item-based recommendations ({n_recs} per item)...")
    
    # Compute item similarity
    item_similarity = compute_similarities(item_embeddings, method='cosine')
    
    recommendations = []
    
    for item_idx_val in range(item_similarity.shape[0]):
        if (item_idx_val + 1) % 1000 == 0:
            logger.info(f"   Progress: {item_idx_val + 1:,} / {item_similarity.shape[0]:,}")
        
        # Get similar items
        similar_indices = np.argsort(item_similarity[item_idx_val])[-n_recs-1:-1]
        
        article_id = list(item_idx.keys())[item_idx_val]
        
        for rank, sim_item_idx in enumerate(similar_indices, 1):
            similar_article_id = list(item_idx.keys())[sim_item_idx]
            score = item_similarity[item_idx_val, sim_item_idx]
            
            recommendations.append({
                'article_id': article_id,
                'similar_article_id': similar_article_id,
                'rank': rank,
                'similarity': float(score),
                'method': 'item_based'
            })
    
    logger.info(f"‚úÖ Generated {len(recommendations):,} item-based recommendations")
    return pd.DataFrame(recommendations)

# ============================================================================
# SAVE OUTPUTS
# ============================================================================

def save_outputs(user_recs, item_recs, user_embeddings, item_embeddings,
                 user_idx, item_idx, users, items):
    """Save all outputs to parquet files"""
    logger.info("üíæ Saving outputs...")
    
    # Save recommendations
    user_recs.to_parquet(OUTPUT_DIR / 'user_based_recommendations.parquet', index=False)
    logger.info(f"   ‚úÖ User-based recommendations ‚Üí {OUTPUT_DIR / 'user_based_recommendations.parquet'}")
    
    item_recs.to_parquet(OUTPUT_DIR / 'item_based_recommendations.parquet', index=False)
    logger.info(f"   ‚úÖ Item-based recommendations ‚Üí {OUTPUT_DIR / 'item_based_recommendations.parquet'}")
    
    # Save embeddings
    pd.DataFrame(user_embeddings).to_parquet(OUTPUT_DIR / 'user_embeddings.parquet', index=False)
    logger.info(f"   ‚úÖ User embeddings ‚Üí {OUTPUT_DIR / 'user_embeddings.parquet'}")
    
    pd.DataFrame(item_embeddings).to_parquet(OUTPUT_DIR / 'item_embeddings.parquet', index=False)
    logger.info(f"   ‚úÖ Item embeddings ‚Üí {OUTPUT_DIR / 'item_embeddings.parquet'}")
    
    # Save mappings
    pd.DataFrame({
        'user_id': list(user_idx.keys()),
        'user_index': list(user_idx.values())
    }).to_parquet(OUTPUT_DIR / 'user_id_mapping.parquet', index=False)
    logger.info(f"   ‚úÖ User ID mapping ‚Üí {OUTPUT_DIR / 'user_id_mapping.parquet'}")
    
    pd.DataFrame({
        'article_id': list(item_idx.keys()),
        'article_index': list(item_idx.values())
    }).to_parquet(OUTPUT_DIR / 'article_id_mapping.parquet', index=False)
    logger.info(f"   ‚úÖ Article ID mapping ‚Üí {OUTPUT_DIR / 'article_id_mapping.parquet'}")

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Main training pipeline"""
    start_time = datetime.now()
    
    logger.info("=" * 80)
    logger.info("COLLABORATIVE FILTERING TRAINING - KAGGLE GPU VERSION")
    logger.info("=" * 80)
    logger.info(f"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"Config: {TRAIN_USER_COUNT:,} users, {N_COMPONENTS} components")
    logger.info("")
    
    try:
        # Load data
        df = load_data()
        
        # Preprocess
        df, user_idx, item_idx, users, items = sample_and_preprocess(df, TRAIN_USER_COUNT)
        
        # Build matrix
        matrix = build_interaction_matrix(df, user_idx, item_idx)
        
        # SVD decomposition
        user_embeddings, item_embeddings, svd = fit_svd(matrix, N_COMPONENTS, SVD_ITERATIONS)
        
        # Build KNN index
        knn_index = build_knn_index(user_embeddings, KNN_NEIGHBORS)
        
        # Generate recommendations
        user_recs = generate_user_based_recommendations(user_embeddings, user_idx, item_idx, matrix, knn_index)
        item_recs = generate_item_based_recommendations(item_embeddings, user_idx, item_idx, matrix)
        
        # Save outputs
        save_outputs(user_recs, item_recs, user_embeddings, item_embeddings, user_idx, item_idx, users, items)
        
        # Summary
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info("")
        logger.info("=" * 80)
        logger.info("‚úÖ TRAINING COMPLETED SUCCESSFULLY")
        logger.info("=" * 80)
        logger.info(f"Total time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)")
        logger.info(f"User-based recommendations: {len(user_recs):,}")
        logger.info(f"Item-based recommendations: {len(item_recs):,}")
        logger.info("")
        logger.info("Next steps:")
        logger.info("1. Download all parquet files from /kaggle/working/")
        logger.info("2. Copy to: data/recommendations/")
        logger.info("3. Run: python ml/recommenders/cf_evaluate.py")
        logger.info("4. Check model quality score")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
