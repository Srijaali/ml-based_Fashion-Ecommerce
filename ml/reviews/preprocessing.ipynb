{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f53ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ND.COM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# NLP libs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import torch\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd86ec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ND.COM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ND.COM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ND.COM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ND.COM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15bcb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96c131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-mul-en'\n",
    "tokenizer_mt = MarianTokenizer.from_pretrained(model_name)\n",
    "model_mt = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d1210b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_english(text):\n",
    "    try:\n",
    "        batch = tokenizer_mt(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        gen = model_mt.generate(**batch)\n",
    "        return tokenizer_mt.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "    except:\n",
    "        return text             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988de0d",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d55e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (300000, 19)\n"
     ]
    }
   ],
   "source": [
    "# Use the current working directory as the base\n",
    "project_root = Path.cwd().parent.parent  # adjust if needed to point to ML_DB_PROJECT\n",
    "\n",
    "# Output directory\n",
    "out_dir = project_root / \"data/ml/reviews/reviews_preprocessed\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load parquet file\n",
    "df = pd.read_parquet(project_root / \"data/ml/reviews/reviews.parquet\")\n",
    "df.fillna(\"\", inplace=True)\n",
    "\n",
    "print(\"Data loaded. Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26acb14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(x):\n",
    "    return BeautifulSoup(x, \"lxml\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64112c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c93f1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b09e0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(x): \n",
    "    return x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd34909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_contractions(x): \n",
    "    return contractions.fix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb71f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [w for w in tokens if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4745d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_spacy(text):\n",
    "    return [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3ecf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_spacy(text):\n",
    "    return \" \".join([token.lemma_ for token in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eac5f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "def split_sentences(text):\n",
    "    try:\n",
    "        return sent_tokenize(text)\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('punkt_tab')\n",
    "        return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ffa8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(x):\n",
    "    try: \n",
    "        return detect(x)\n",
    "    except:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4584d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_spam(text):\n",
    "    if len(text.split()) < 2: \n",
    "        return True\n",
    "    if len(set(text.split())) < 2: \n",
    "        return True\n",
    "    if re.search(r\"(.)\\1{3,}\", text): \n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67ce4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_clean(text):\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e8111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = []\n",
    "tokens_list = []\n",
    "lemmas_list = []\n",
    "sentences_list = []\n",
    "languages = []\n",
    "translations = []\n",
    "spam_flags = []\n",
    "final_clean_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26a48095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [5:58:03<00:00, 13.96it/s]     \n"
     ]
    }
   ],
   "source": [
    "for txt in tqdm(df.review_text.tolist()):\n",
    "\n",
    "    t = txt\n",
    "\n",
    "    # 1 remove html, emoji, special chars\n",
    "    t = remove_html(t)\n",
    "    t = remove_emojis(t)\n",
    "    t = remove_special_chars(t)\n",
    "\n",
    "    # 2 lowercase\n",
    "    t = lower(t)\n",
    "\n",
    "    # 3 contractions\n",
    "    t = fix_contractions(t)\n",
    "\n",
    "    # 5 tokenization\n",
    "    tokens = tokenize_spacy(t)\n",
    "\n",
    "    # 4 stopwords\n",
    "    tokens_nostop = remove_stopwords(tokens)\n",
    "\n",
    "    # 6 lemmatization\n",
    "    lemma_text = lemmatize_spacy(\" \".join(tokens_nostop))\n",
    "\n",
    "    # 7 sentence splitting\n",
    "    sents = split_sentences(lemma_text)\n",
    "\n",
    "    # 8 language detection\n",
    "    lang = detect_language(txt)\n",
    "    if lang != \"en\":\n",
    "        translated = translate_to_english(txt)\n",
    "    else:\n",
    "        translated = t\n",
    "\n",
    "    # 9 spam detection\n",
    "    spam = detect_spam(lemma_text)\n",
    "\n",
    "    # 10 deep cleaning\n",
    "    cleaned_final = deep_clean(translated)\n",
    "\n",
    "    clean_texts.append(t)\n",
    "    tokens_list.append(tokens_nostop)\n",
    "    lemmas_list.append(lemma_text)\n",
    "    sentences_list.append(sents)\n",
    "    languages.append(lang)\n",
    "    translations.append(translated)\n",
    "    spam_flags.append(spam)\n",
    "    final_clean_text.append(cleaned_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91aa8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = clean_texts\n",
    "df[\"tokens\"] = tokens_list\n",
    "df[\"lemmas\"] = lemmas_list\n",
    "df[\"sentences\"] = sentences_list\n",
    "df[\"language_detected\"] = languages\n",
    "df[\"translated_text\"] = translations\n",
    "df[\"is_spam\"] = spam_flags\n",
    "df[\"final_text_for_ml\"] = final_clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d687319e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_id                    0\n",
       "customer_id                  0\n",
       "article_id                   0\n",
       "category_id                  0\n",
       "rating                       0\n",
       "review_text                  0\n",
       "created_at                   0\n",
       "verified_purchase            0\n",
       "helpful_votes                0\n",
       "synthetic_sentiment_label    0\n",
       "aspect_terms                 0\n",
       "language                     0\n",
       "review_length                0\n",
       "review_source                0\n",
       "review_age_days              0\n",
       "clean_text                   0\n",
       "vader_score                  0\n",
       "vader_label                  0\n",
       "aspect_terms_list            0\n",
       "tokens                       0\n",
       "lemmas                       0\n",
       "sentences                    0\n",
       "language_detected            0\n",
       "translated_text              0\n",
       "is_spam                      0\n",
       "final_text_for_ml            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a3b3e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# impute missing values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df.fillna({\u001b[33m\"\u001b[39m\u001b[33mtranslated_text\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfinal_text_for_ml\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ND.COM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:8160\u001b[39m, in \u001b[36mNDFrame.replace\u001b[39m\u001b[34m(self, to_replace, value, inplace, limit, regex, method)\u001b[39m\n\u001b[32m   8154\u001b[39m         new_data = \u001b[38;5;28mself\u001b[39m._mgr.replace_regex(\n\u001b[32m   8155\u001b[39m             to_replace=to_replace,\n\u001b[32m   8156\u001b[39m             value=value,\n\u001b[32m   8157\u001b[39m             inplace=inplace,\n\u001b[32m   8158\u001b[39m         )\n\u001b[32m   8159\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m8160\u001b[39m         new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   8161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mto_replace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_replace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\n\u001b[32m   8162\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   8164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   8165\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInvalid \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto_replace\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(to_replace).\u001b[34m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   8166\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ND.COM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\base.py:249\u001b[39m, in \u001b[36mDataManager.replace\u001b[39m\u001b[34m(self, to_replace, value, inplace)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.is_list_like(to_replace)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.is_list_like(value)\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreplace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_replace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_replace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ND.COM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ND.COM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:895\u001b[39m, in \u001b[36mBlock.replace\u001b[39m\u001b[34m(self, to_replace, value, inplace, mask, using_cow, already_warned, convert_string)\u001b[39m\n\u001b[32m    892\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.copy()]\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     mask = \u001b[43mmissing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmask_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_replace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mask.any():\n\u001b[32m    897\u001b[39m     \u001b[38;5;66;03m# Note: we get here with test_replace_extension_other incorrectly\u001b[39;00m\n\u001b[32m    898\u001b[39m     \u001b[38;5;66;03m#  bc _can_hold_element is incorrect.\u001b[39;00m\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m using_cow:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ND.COM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\missing.py:124\u001b[39m, in \u001b[36mmask_missing\u001b[39m\u001b[34m(arr, values_to_mask)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m potential_na:\n\u001b[32m    123\u001b[39m     new_mask = np.zeros(arr.shape, dtype=np.bool_)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     new_mask[arr_mask] = \u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43marr_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    126\u001b[39m     new_mask = arr == x\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# impute missing values\n",
    "df.replace(\"\", np.nan, inplace=True)\n",
    "df.fillna({\"translated_text\": \"\", \"final_text_for_ml\": \"\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b966f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_for_training\"] = df[\"final_text_for_ml\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eda329bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review_id', 'customer_id', 'article_id', 'category_id', 'rating', 'review_text', 'created_at', 'verified_purchase', 'helpful_votes', 'synthetic_sentiment_label', 'aspect_terms', 'language', 'review_length', 'review_source', 'review_age_days', 'clean_text', 'vader_score', 'vader_label', 'aspect_terms_list', 'tokens', 'lemmas', 'sentences', 'language_detected', 'translated_text', 'is_spam', 'final_text_for_ml', 'text_for_training']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f231a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {'neutral': 1.667834150572067, 'positive': 0.5121979952570466, 'negative': 2.2318937618569357}\n"
     ]
    }
   ],
   "source": [
    "# Balancing — Transformers use class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = df[\"synthetic_sentiment_label\"].unique()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=df[\"synthetic_sentiment_label\"])\n",
    "print(\"Class Weights:\", dict(zip(classes, class_weights)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "659f4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, temp = train_test_split(df, test_size=0.3, stratify=df[\"synthetic_sentiment_label\"], random_state=42)\n",
    "val, test = train_test_split(temp, test_size=0.5, stratify=temp[\"synthetic_sentiment_label\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fbddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved successfully to:\n",
      "C:\\Users\\ND.COM\\Desktop\\ML DB Project\\data\\ml\\reviews\\reviews_preprocessed\n"
     ]
    }
   ],
   "source": [
    "train.to_parquet(out_dir / \"train.parquet\", index=False)\n",
    "val.to_parquet(out_dir / \"val.parquet\", index=False)\n",
    "test.to_parquet(out_dir / \"test.parquet\", index=False)\n",
    "df.to_parquet(out_dir / \"reviews_preprocessed.parquet\", index=False)\n",
    "\n",
    "print(f\"All files saved successfully to:\\n{out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c37208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Rows: 300000\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing complete. Rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df210d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
