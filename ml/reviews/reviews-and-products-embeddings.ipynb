{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14063019,"sourceType":"datasetVersion","datasetId":8946228},{"sourceId":14065487,"sourceType":"datasetVersion","datasetId":8952836}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================\n# PHASE 3 — REVIEW & PRODUCT EMBEDDINGS (KAGGLE 2025 — FULLY FIXED)\n# Fixes NumPy 2.2.6 binary incompatibility + creates reusable dataset\n# Runtime: ~15–20 min on 300k reviews (GPU)\n# ================================\n\n# %% [markdown]\n# ## 1) CRITICAL FIX: Downgrade NumPy to 1.26.4 (solves dtype size error)\n# Kaggle's NumPy 2.2.6 breaks sklearn/transformers — this forces 1.x compatibility\n# %%\n!pip uninstall -y numpy scipy scikit-learn transformers sentence-transformers torch torchvision torchaudio\n!pip install \"numpy==1.26.4\" \"scipy==1.13.1\" \"scikit-learn==1.3.2\" --no-cache-dir --force-reinstall\n\n# %% [markdown]\n# ## 2) Install the rest (now compatible)\n# %%\n!pip install -q sentence-transformers==2.3.1 faiss-cpu==1.8.0 pandas==2.1.4 tqdm\n\n# %% [markdown]\n# ## 3) FORCE KERNEL RESTART (required after downgrade)\n# Run this cell — Kaggle will restart automatically\n# %%\nimport os\nos.kill(os.getpid(), 9)  # Clean restart to load new NumPy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:20:09.305041Z","iopub.execute_input":"2025-12-08T17:20:09.305390Z","execution_failed":"2025-12-08T17:25:20.005Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nFound existing installation: scipy 1.15.3\nUninstalling scipy-1.15.3:\n  Successfully uninstalled scipy-1.15.3\nFound existing installation: scikit-learn 1.2.2\nUninstalling scikit-learn-1.2.2:\n  Successfully uninstalled scikit-learn-1.2.2\nFound existing installation: transformers 4.53.3\nUninstalling transformers-4.53.3:\n  Successfully uninstalled transformers-4.53.3\nFound existing installation: sentence-transformers 4.1.0\nUninstalling sentence-transformers-4.1.0:\n  Successfully uninstalled sentence-transformers-4.1.0\nFound existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nCollecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy==1.13.1\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scikit-learn==1.3.2\n  Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting joblib>=1.1.1 (from scikit-learn==1.3.2)\n  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\nCollecting threadpoolctl>=2.0.0 (from scikit-learn==1.3.2)\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m240.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m220.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m229.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m279.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n  Attempting uninstall: threadpoolctl\n    Found existing installation: threadpoolctl 3.6.0\n    Uninstalling threadpoolctl-3.6.0:\n      Successfully uninstalled threadpoolctl-3.6.0\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.5.2\n    Uninstalling joblib-1.5.2:\n      Successfully uninstalled joblib-1.5.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchmetrics 1.8.2 requires torch>=2.0.0, which is not installed.\neasyocr 1.7.2 requires torch, which is not installed.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\npytorch-lightning 2.5.5 requires torch>=2.1.0, which is not installed.\nfastai 2.8.5 requires torch<2.10,>=1.10, which is not installed.\nfastai 2.8.5 requires torchvision>=0.11, which is not installed.\nstable-baselines3 2.1.0 requires torch>=1.13, which is not installed.\nkaggle-environments 1.18.0 requires transformers>=4.33.1, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npeft 0.16.0 requires torch>=1.13.0, which is not installed.\npeft 0.16.0 requires transformers, which is not installed.\naccelerate 1.9.0 requires torch>=2.0.0, which is not installed.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed joblib-1.5.2 numpy-1.26.4 scikit-learn-1.3.2 scipy-1.13.1 threadpoolctl-3.6.0\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m541.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\nfastai 2.8.5 requires torchvision>=0.11, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ntimm 1.0.19 requires torchvision, which is not installed.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nxarray 2025.7.1 requires pandas>=2.2, but you have pandas 2.1.4 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# PHASE 3 — REVIEW & PRODUCT EMBEDDINGS (KAGGLE VERSION)\n# Generates: review embeddings + product embeddings + FAISS indices\n# Output: Ready-to-use dataset for recommendations, similarity, clustering\n# Runtime: ~12–18 min on 300k reviews (GPU)\n# ================================\n\n# %% [markdown]\n# ## Install dependencies\n# %%\n!pip install -q sentence-transformers faiss-cpu # faiss-gpu is faster on Kaggle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T16:58:15.532224Z","iopub.execute_input":"2025-12-08T16:58:15.532633Z","iopub.status.idle":"2025-12-08T16:58:21.523318Z","shell.execute_reply.started":"2025-12-08T16:58:15.532607Z","shell.execute_reply":"2025-12-08T16:58:21.522412Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:30:35.355781Z","iopub.execute_input":"2025-12-08T17:30:35.356068Z","iopub.status.idle":"2025-12-08T17:30:41.468128Z","shell.execute_reply.started":"2025-12-08T17:30:35.356047Z","shell.execute_reply":"2025-12-08T17:30:41.467019Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.13.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sentence_transformers import SentenceTransformer\nimport faiss\n\n# Auto-find your preprocessed reviews (from Phase 2)\nINPUT_DIR = Path(\"/kaggle/input\")\nWORKING_DIR = Path(\"/kaggle/working\")\nOUTPUT_DIR = WORKING_DIR / \"embeddings_output\"\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Find the folder containing reviews_features_full.parquet\nparquet_files = list(INPUT_DIR.rglob(\"reviews_features_full.parquet\"))\nif not parquet_files:\n    raise FileNotFoundError(\"reviews_features_full.parquet not found! Add your Phase 2 dataset.\")\nDATA_DIR = parquet_files[0].parent\nprint(f\"Found data at: {DATA_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:30:44.819114Z","iopub.execute_input":"2025-12-08T17:30:44.819467Z","iopub.status.idle":"2025-12-08T17:30:44.884299Z","shell.execute_reply.started":"2025-12-08T17:30:44.819436Z","shell.execute_reply":"2025-12-08T17:30:44.883419Z"}},"outputs":[{"name":"stdout","text":"Found data at: /kaggle/input/your-reviews-features-full\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ## 1) Load reviews\n# %%\ndf = pd.read_parquet(DATA_DIR / \"reviews_features_full.parquet\")\nprint(f\"Loaded {len(df):,} reviews\")\n\ntexts = df['text_for_training'].fillna('').astype(str).tolist()\narticle_ids = df['article_id'].astype(str).tolist()\nreview_ids = df['review_id'].astype(str).tolist()  # keep as string to avoid overflow\n\nprint(\"Sample text:\", texts[0][:200])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:30:47.014575Z","iopub.execute_input":"2025-12-08T17:30:47.014931Z","iopub.status.idle":"2025-12-08T17:30:48.567713Z","shell.execute_reply.started":"2025-12-08T17:30:47.014905Z","shell.execute_reply":"2025-12-08T17:30:48.566767Z"}},"outputs":[{"name":"stdout","text":"Loaded 300,000 reviews\nSample text: average product feels okay might be fine for everyday use\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# %% [markdown]\n# ## 2) Load Sentence-BERT Model (best speed/quality)\n# %%\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # uses GPU automatically\nprint(\"Model loaded: all-MiniLM-L6-v2 (384-dim)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:30:50.982609Z","iopub.execute_input":"2025-12-08T17:30:50.982935Z","iopub.status.idle":"2025-12-08T17:31:00.198897Z","shell.execute_reply.started":"2025-12-08T17:30:50.982911Z","shell.execute_reply":"2025-12-08T17:31:00.197996Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca75c01d9fd4f92b81316e56c5e6101"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72ffda90138461ab3bb93dc6ae220ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"312c608419594957bce366870a61fd2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7fbf922f6324da198b190f8f3926dc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bbf2e6de53a42a4a4c13c11a34fc2b3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b3bf077961440f28c96bcc4c3a60952"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"455b5b54fe124d6abf3b1c2a5d51953d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef217c8dddf945e889a2396340e5a79e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a3202b16e6b434383505081dfde5104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67a7cee6dd704027a83ed00cc8b0079f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b7d5e67d97d40fca21f9c63f7e48272"}},"metadata":{}},{"name":"stdout","text":"Model loaded: all-MiniLM-L6-v2 (384-dim)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ================================\n\n\n# %% [markdown]\n# ## 3) Compute Review Embeddings (batched + fast)\n# %%\nbatch_size = 512\nembeddings_list = []\n\nprint(\"Encoding reviews in batches...\")\nfor i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n    batch = texts[i:i + batch_size]\n    batch_emb = model.encode(\n        batch,\n        batch_size=batch_size,\n        show_progress_bar=False,\n        convert_to_numpy=True,\n        normalize_embeddings=True  # crucial for cosine similarity\n    )\n    embeddings_list.append(batch_emb)\n\nreview_embeddings = np.vstack(embeddings_list)\nprint(f\"Review embeddings shape: {review_embeddings.shape}\")\n\n# Save\nreview_emb_path = OUTPUT_DIR / \"review_embeddings.npy\"\nnp.save(review_emb_path, review_embeddings)\nprint(f\"Saved to {review_emb_path}\")\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:31:04.909102Z","iopub.execute_input":"2025-12-08T17:31:04.909456Z","iopub.status.idle":"2025-12-08T17:53:33.705965Z","shell.execute_reply.started":"2025-12-08T17:31:04.909431Z","shell.execute_reply":"2025-12-08T17:53:33.705104Z"}},"outputs":[{"name":"stdout","text":"Encoding reviews in batches...\n","output_type":"stream"},{"name":"stderr","text":"Encoding: 100%|██████████| 586/586 [22:28<00:00,  2.30s/it]\n","output_type":"stream"},{"name":"stdout","text":"Review embeddings shape: (300000, 384)\nSaved to /kaggle/working/embeddings_output/review_embeddings.npy\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# %% [markdown]\n# ## 4) Build FAISS Index for Reviews (instant similarity search)\n# %%\nd = review_embeddings.shape[1]\nindex_reviews = faiss.IndexFlatIP(d)  # Inner Product = Cosine (because normalized)\nindex_reviews.add(review_embeddings.astype('float32'))\n\nfaiss.write_index(index_reviews, str(OUTPUT_DIR / \"faiss_review_index.index\"))\nprint(\"FAISS review index built and saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:53:42.145258Z","iopub.execute_input":"2025-12-08T17:53:42.145654Z","iopub.status.idle":"2025-12-08T17:53:43.156044Z","shell.execute_reply.started":"2025-12-08T17:53:42.145619Z","shell.execute_reply":"2025-12-08T17:53:43.155028Z"}},"outputs":[{"name":"stdout","text":"FAISS review index built and saved\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# %% [markdown]\n# ## 5) Product-Level Embeddings (average pooling) — FIXED & MEMORY SAFE\n# %%\nimport gc\n\nprint(\"Computing product-level embeddings (average of review embeddings)...\")\n\n# Free up RAM first (review_embeddings can be huge)\nif 'review_embeddings' in globals():\n    del review_embeddings\n    gc.collect()\n\n# We will use the saved .npy file in memory-mapped mode → no OOM\nreview_emb_path = OUTPUT_DIR / \"review_embeddings.npy\"\nreview_embeddings_mmap = np.load(review_emb_path, mmap_mode='r')  # ← loads without eating RAM\n\n# Create a mapping: article_id → list of row indices in the embeddings file\ndf_with_idx = pd.DataFrame({\n    'article_id': article_ids,\n    'emb_idx': np.arange(len(article_ids))   # row index = position in embeddings file\n})\n\n# Group indices per product\ngrouped_indices = df_with_idx.groupby('article_id')['emb_idx'].apply(list)\n\nprint(f\"Found {len(grouped_indices):,} unique products → averaging reviews...\")\n\nproduct_embeddings_list = []\nproduct_ids_list = []\n\nfor article_id, idx_list in tqdm(grouped_indices.items(), desc=\"Averaging per product\"):\n    # Load only the embeddings for this product (still memory-mapped → very cheap)\n    product_embs = review_embeddings_mmap[idx_list]           # shape: (n_reviews, 384)\n    avg_emb = np.mean(product_embs, axis=0).astype('float32') # average → 384-dim\n    product_embeddings_list.append(avg_emb)\n    product_ids_list.append(article_id)\n\n# Convert to final matrix\nproduct_matrix = np.stack(product_embeddings_list)  # shape: (n_products, 384)\nproduct_ids = product_ids_list\n\nprint(f\"Product embeddings ready → shape: {product_matrix.shape}\")\n\n# Save\nnp.save(OUTPUT_DIR / \"product_embeddings.npy\", product_matrix)\npd.DataFrame({'article_id': product_ids}).to_parquet(OUTPUT_DIR / \"product_ids.parquet\", index=False)\nprint(\"Product embeddings + IDs saved!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:56:10.525320Z","iopub.execute_input":"2025-12-08T17:56:10.526048Z","iopub.status.idle":"2025-12-08T17:56:11.634717Z","shell.execute_reply.started":"2025-12-08T17:56:10.526022Z","shell.execute_reply":"2025-12-08T17:56:11.633773Z"}},"outputs":[{"name":"stdout","text":"Computing product-level embeddings (average of review embeddings)...\nFound 6,292 unique products → averaging reviews...\n","output_type":"stream"},{"name":"stderr","text":"Averaging per product: 6292it [00:00, 14634.86it/s]","output_type":"stream"},{"name":"stdout","text":"Product embeddings ready → shape: (6292, 384)\nProduct embeddings + IDs saved!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# %% [markdown]\n# ## 6) FAISS Index for Products\n# %%\nindex_products = faiss.IndexFlatIP(d)\nindex_products.add(product_matrix)\n\nfaiss.write_index(index_products, str(OUTPUT_DIR / \"faiss_product_index.index\"))\nprint(\"FAISS product index saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:56:22.967178Z","iopub.execute_input":"2025-12-08T17:56:22.967517Z","iopub.status.idle":"2025-12-08T17:56:22.983384Z","shell.execute_reply.started":"2025-12-08T17:56:22.967471Z","shell.execute_reply":"2025-12-08T17:56:22.982529Z"}},"outputs":[{"name":"stdout","text":"FAISS product index saved\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# %% [markdown]\n# ## 7) Demo: Similar Products\n# %%\ndef get_similar_products(article_id, k=6):\n    if article_id not in product_ids:\n        return f\"Product {article_id} not found\"\n    idx = product_ids.index(article_id)\n    query = product_matrix[idx:idx+1]\n    scores, indices = index_products.search(query, k+1)  # +1 to skip itself\n    similar_ids = [product_ids[i] for i in indices[0][1:]]  # skip first (itself)\n    return similar_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:56:26.252592Z","iopub.execute_input":"2025-12-08T17:56:26.252898Z","iopub.status.idle":"2025-12-08T17:56:26.258514Z","shell.execute_reply.started":"2025-12-08T17:56:26.252876Z","shell.execute_reply":"2025-12-08T17:56:26.257406Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Test\nsample_id = product_ids[0]\nprint(f\"Sample product: {sample_id}\")\nprint(\"Similar products:\", get_similar_products(sample_id, k=5))\n\n# %% [markdown]\n# ## 8) Save Everything as a Reusable Kaggle Dataset\n# %%\nprint(\"Creating final dataset...\")\n\n!mkdir -p /kaggle/working/final_embeddings_dataset\n!cp -r {OUTPUT_DIR}/* /kaggle/working/final_embeddings_dataset/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:56:29.146233Z","iopub.execute_input":"2025-12-08T17:56:29.146608Z","iopub.status.idle":"2025-12-08T17:56:30.471086Z","shell.execute_reply.started":"2025-12-08T17:56:29.146587Z","shell.execute_reply":"2025-12-08T17:56:30.470325Z"}},"outputs":[{"name":"stdout","text":"Sample product: 176209023\nSimilar products: ['564474001', '557248016', '554126001', '493814030', '488555002']\nCreating final dataset...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Create the dataset (run once!)\n!kaggle datasets create -p /kaggle/working/final_embeddings_dataset --dir-mode tar -m \"Review + Product embeddings + FAISS indices (all-MiniLM-L6-v2)\"\n\nprint(\"\\nDONE! Your dataset is created.\")\nprint(\"Files included:\")\n!ls -lh /kaggle/working/final_embeddings_dataset/\n\n# %% [markdown]\n# ## What You Get (add this dataset to any future notebook)\n# ├── review_embeddings.npy\n# ├── product_embeddings.npy\n# ├── product_ids.parquet\n# ├── faiss_review_index.index\n# └── faiss_product_index.index\n# \n# → Instant \"You might also like\", user personalization, cold-start solution, clustering, etc.\n# ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T17:56:41.300377Z","iopub.execute_input":"2025-12-08T17:56:41.300733Z","iopub.status.idle":"2025-12-08T17:56:42.380667Z","shell.execute_reply.started":"2025-12-08T17:56:41.300705Z","shell.execute_reply":"2025-12-08T17:56:42.379349Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/usr/local/bin/kaggle\", line 4, in <module>\n    from kaggle.cli import main\n  File \"/usr/local/lib/python3.11/dist-packages/kaggle/__init__.py\", line 6, in <module>\n    api.authenticate()\n  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n    raise IOError('Could not find {}. Make sure it\\'s located in'\nOSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n\nDONE! Your dataset is created.\nFiles included:\ntotal 898M\n-rw-r--r-- 1 root root 9.3M Dec  8 17:56 faiss_product_index.index\n-rw-r--r-- 1 root root 440M Dec  8 17:56 faiss_review_index.index\n-rw-r--r-- 1 root root 9.3M Dec  8 17:56 product_embeddings.npy\n-rw-r--r-- 1 root root  42K Dec  8 17:56 product_ids.parquet\n-rw-r--r-- 1 root root 440M Dec  8 17:56 review_embeddings.npy\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# %% [markdown]\n# CREATE review_ids.parquet — Maps embedding row index → original review_id\n# Run this in the same notebook where you generated review_embeddings.npy\n# %%\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ———————— CONFIG: UPDATE THESE PATHS FOR YOUR NOTEBOOK ————————\n# Change these only if your files are in different locations\n\n# 1) Path to your preprocessed reviews (the one used for embedding generation)\nPREPROCESSED_PATH = Path(\"/kaggle/input/your-reviews-features-full/reviews_features_full.parquet\")\n# Common alternatives:\n# PREPROCESSED_PATH = Path(\"/kaggle/input/hm-reviews-preprocessed/reviews_preprocessed.parquet\")\n# PREPROCESSED_PATH = Path(\"/kaggle/working/data/ml/reviews_preprocessed.parquet\")\n\n# 2) Where to save the mapping (same folder as your embeddings)\nOUTPUT_DIR = Path(\"/kaggle/working/embeddings\")        # ← most common\n# Or if you used a different folder:\n# OUTPUT_DIR = Path(\"/kaggle/working/embeddings_output\")\n\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ———————————————————————————————————————————————————————————————\n\n# Load the dataframe that was used to generate embeddings\nprint(f\"Loading reviews from: {PREPROCESSED_PATH}\")\ndf = pd.read_parquet(PREPROCESSED_PATH)\n\nprint(f\"Loaded {len(df):,} reviews\")\n\n# Ensure review_id exists and is unique\nif \"review_id\" not in df.columns:\n    raise ValueError(\"review_id column not found! Check your parquet file.\")\nif df[\"review_id\"].duplicated().any():\n    print(\"Warning: review_id has duplicates — taking first occurrence\")\n\n# Create mapping: embedding row index (0, 1, 2, ...) → review_id\nmapping_df = pd.DataFrame({\n    \"idx\": np.arange(len(df)),           # This matches the row order in review_embeddings.npy\n    \"review_id\": df[\"review_id\"].astype(str).values\n})\n\n# Optional: also save article_id if useful later\nmapping_df[\"article_id\"] = df[\"article_id\"].astype(str).values\n\n# Save\noutput_path = OUTPUT_DIR / \"review_ids.parquet\"\nmapping_df.to_parquet(output_path, index=False)\n\nprint(f\"Saved mapping → {output_path}\")\nprint(f\"Shape: {mapping_df.shape}\")\nprint(\"\\nFirst 5 rows:\")\nprint(mapping_df.head())\n\n# Bonus: Add to your final dataset so it's always together\n!cp {output_path} /kaggle/working/final_embeddings_dataset/ 2>/dev/null || echo \"Skip copy if folder doesn't exist\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T20:40:59.495225Z","iopub.execute_input":"2025-12-08T20:40:59.495488Z","iopub.status.idle":"2025-12-08T20:41:00.924790Z","shell.execute_reply.started":"2025-12-08T20:40:59.495471Z","shell.execute_reply":"2025-12-08T20:41:00.923773Z"}},"outputs":[{"name":"stdout","text":"Loading reviews from: /kaggle/input/your-reviews-features-full/reviews_features_full.parquet\nLoaded 300,000 reviews\nSaved mapping → /kaggle/working/embeddings/review_ids.parquet\nShape: (300000, 3)\n\nFirst 5 rows:\n   idx review_id article_id\n0    0   2116198  399061015\n1    1   2116199  789274001\n2    2   2116200  399223001\n3    3   2116201  393447016\n4    4   2116202  399223034\nSkip copy if folder doesn't exist\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"emb = np.load(\"/kaggle/input/model-emb/review_embeddings.npy\")\nmapdf = pd.read_parquet(\"/kaggle/working/embeddings/review_ids.parquet\")\n\nprint(len(emb), len(mapdf))\nassert len(emb) == len(mapdf), \"Mismatch! Wrong dataframe used.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T20:55:11.317785Z","iopub.execute_input":"2025-12-08T20:55:11.318055Z","iopub.status.idle":"2025-12-08T20:55:11.585143Z","shell.execute_reply.started":"2025-12-08T20:55:11.318035Z","shell.execute_reply":"2025-12-08T20:55:11.584075Z"}},"outputs":[{"name":"stdout","text":"300000 300000\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}